{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "313e405f",
   "metadata": {},
   "source": [
    "Q-1 create a 1D array of 9 elements using numpy module and reshape it into the 2D array of size 3*3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e750e01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8]\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x= np.arange(9)\n",
    "print(x)\n",
    "y= x.reshape(3,3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d725afcb",
   "metadata": {},
   "source": [
    "Q-2 Print the output of the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839c9e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "list3=[x for x in range (10) if x%2==0]\n",
    "print(list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e887921",
   "metadata": {},
   "source": [
    "Q-3 Given the string text = \"PythonProgramming\". Perform string slicing to extract specific substrings according to the following instructions: a) Slice the string to obtain the first 6 characters. b) Extract a substring\n",
    "that includes the characters from index 6 to index 13. c) Slice the last 5 characters from the string. d) Create a new string by slicing and concatenating the first 4 characters and the last 3 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4cdebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "Programm\n",
      "mming\n",
      "Pything\n"
     ]
    }
   ],
   "source": [
    "x=\"PythonProgramming\"\n",
    "y=x[:6]\n",
    "print(y)\n",
    "z=x[6:14]\n",
    "print(z)\n",
    "m=x[-5:]\n",
    "print(m)\n",
    "t=x[:4]+x[-3:]\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2867d",
   "metadata": {},
   "source": [
    "Q-5 Write a python program to generate two DataFrames, namely, di and d2. Construct di utilizing a two-dimensional list, and create d2 using a dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38dfb391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2\n",
      "0  1  2  3\n",
      "1  4  5  6\n",
      "    Name  Age\n",
      "0  chery   20\n",
      "1    tej   28\n",
      "2    ram   32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "list1 = [[1,2,3],[4,5,6]]\n",
    "dic1 = {'Name': ['chery','tej','ram'],\n",
    " 'Age':[20,28,32],\n",
    " }\n",
    "d1 = pd.DataFrame(list1)\n",
    "d2 = pd.DataFrame(dic1)\n",
    "print(d1)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b867bec",
   "metadata": {},
   "source": [
    "Q-6 How to measure strength of association between two variables? Write a python code to discuss in detail about the variance, standard deviation. covariance, and correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48b32808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "50.0\n",
      "7.0710678118654755\n",
      "7.0710678118654755\n",
      "62.5\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data={\n",
    " 'x':[10,15,20,25,30],\n",
    " 'y':[5,10,15,20,25]\n",
    "}\n",
    "df=pd.DataFrame(data) \n",
    "variance_x=np.var(df['x'])\n",
    "variance_y=np.var(df['y'])\n",
    "dev_x=np.std(df['x'])\n",
    "dev_y=np.std(df['y'])\n",
    "covariance=np.cov(df['x'],df['y'])[0,1]\n",
    "correlation=np.corrcoef(df['x'],df['y'])[0,1]\n",
    "print(variance_x)\n",
    "print(variance_y)\n",
    "print(dev_x)\n",
    "print(dev_y)\n",
    "print(covariance)\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ba99a",
   "metadata": {},
   "source": [
    "Q-7 Write a python program to explain the concepts of standardization and normalization? Discuss the circumstances under which it is appropriate to utilize these techniques in data preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4235702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after standardization:\n",
      "[[-1.22474487 -1.22474487 -1.22474487]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 1.22474487  1.22474487  1.22474487]]\n",
      "\n",
      "Data after normalization:\n",
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Sample data\n",
    "data = np.array([[1, 2, 3],\n",
    " [4, 5, 6],\n",
    " [7, 8, 9]])\n",
    "# Standardization\n",
    "scaler_std = StandardScaler()\n",
    "data_std = scaler_std.fit_transform(data)\n",
    "print(\"Data after standardization:\")\n",
    "print(data_std)\n",
    "print()\n",
    "# Normalization\n",
    "scaler_norm = MinMaxScaler()\n",
    "data_norm = scaler_norm.fit_transform(data)\n",
    "print(\"Data after normalization:\")\n",
    "print(data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a3c0d",
   "metadata": {},
   "source": [
    "Q-10 Provide a detailed explanation of the PCA technique for dimensionality reduction, including its methodology and application, supported by an illustrative example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66902495",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a powerful technique used in data analysis, particularly for reducing the dimensionality of datasets while preserving crucial information. It does this by transforming the original\n",
    "variables into a set of new, uncorrelated variables called principal components. Here’s a breakdown of PCA’s key aspects:\n",
    "Dimensionality Reduction: PCA helps manage high-dimensional datasets by extracting essential information and discarding less relevant features, simplifying analysis. Data Exploration and Visualization: It plays a\n",
    "significant role in data exploration and visualization, aiding in uncovering hidden patterns and insights. Linear Transformation: PCA performs a linear transformation of data, seeking directions of maximum variance.\n",
    "Feature Selection: Principal components are ranked by the variance they explain, allowing for effective feature selection. Data Compression: PCA can compress data while preserving most of the original\n",
    "information. Clustering and Classification: It finds applications in clustering and classification tasks by reducing noise and highlighting underlying structure. Advantages: PCA offers linearity, computational efficiency,\n",
    "and scalability for large datasets. Limitations: It assumes data normality and linearity and may lead to information loss. Let’s say we have a data set of dimension 300 (n) × 50 (p). n represents the number of\n",
    "observations, and p represents the number of predictors. Since we have a large p = 50, there can be p(p-1)/2 scatter plots, i.e., more than 1000 plots possible to analyze the variable relationship. Wouldn’t it be a\n",
    "tedious job to perform exploratory analysis on this data?\n",
    "In this case, it would be a lucid approach to select a subset of p (p << 50) predictor which captures so much information, followed by plotting the observation in the resultant low-dimensional space.\n",
    "The image below shows the transformation of high-dimensional data (3 dimension) to low-dimensional data (2 dimension) using PCA. Not to forget, each resultant dimension is a linear combination of p features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5918c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48ba8cb1",
   "metadata": {},
   "source": [
    "Q-12 explain the conceptof hypothesis testing in statical analysis? define the p-value in the context of hypothesis testing and explain significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92586f",
   "metadata": {},
   "source": [
    "Hypothesis testing is a form of statistical inference that uses data from a sample to draw conclusions about a population parameter or a population probability distribution. First, a tentative assumption is made about\n",
    "the parameter or distribution. The P value is defined as the probability under the assumption of no effect or no difference (null hypothesis), of obtaining a result equal to or more extreme than what was actually\n",
    "observed. The P stands for probability and measures how likely it is that any observed difference between groups is due to chance. P Value and Statistical Significance: An Uncommon Ground"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
